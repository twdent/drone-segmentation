{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"qWWorEfvGrB1"},"source":["# UAVid Drone Dataset\n","\n","\n","UAVid Dataset\n","The The UAVid dataset is an UAV video dataset for semantic segmentation task focusing on urban scenes. It has several features:\n","\n","Semantic segmentation 4K resolution UAV videos 8 object categories Street scene context.\n","\n","The segmentation categories need to be converted to the format of the Semantic Drone Dataset, as seen below\n","\n","## Semantic Annotation\n","\n","The images are labeled densely using polygons and contain the following 24 classes: \n","  - unlabeled\n","  - paved-area\n","  - dirt\n","  - grass\n","  - gravel\n","  - water\n","  - rocks\n","  - pool\n","  - vegetation\n","  - roof\n","  - wall\n","  - window\n","  - door\n","  - fence\n","  - fence-pole\n","  - person\n","  - dog\n","  - car\n","  - bicycle\n","  - tree\n","  - bald-tree\n","  - ar-marker\n","  - obstacle\n","  - conflicting"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3668,"status":"ok","timestamp":1681301944548,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"8wCHkWIWGrB8","scrolled":false,"trusted":true},"outputs":[],"source":["import cv2\n","import random\n","import albumentations as A\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import os\n","\n","#if running locally\n","drive_root = \".\" "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"y9CkscNCGrB8"},"source":["# Data Augmentation using Albumentations Library - Assumed Complete\n","\n","Perfromed as described in README.md"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"f-T1zysSGrCD"},"source":["# Working with Augmented Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1681301968115,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"RodkZow-GrCE","scrolled":false,"trusted":true},"outputs":[],"source":["import keract\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from IPython.display import SVG\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import os, re, sys, random, shutil, cv2\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import applications, optimizers\n","from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras.utils import model_to_dot, plot_model\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler, TensorBoard\n","from tensorflow.keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, BatchNormalization, Add, concatenate, Conv2DTranspose"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1681301970660,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"rxvF3fv5GrCE","scrolled":false,"trusted":true},"outputs":[],"source":["dataset = 'semantic-drone' # 'uavid' or 'semantic-drone'\n","train_images = drive_root + f'/augmented-{dataset}-dataset/train_images/'\n","train_masks = drive_root + f'/augmented-{dataset}-dataset/train_masks/'\n","val_images = drive_root + f'/augmented-{dataset}-dataset/val_images/'\n","val_masks = drive_root + f'/augmented-{dataset}-dataset/val_masks/'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Labels for Augmented Images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class_dict_df = pd.read_csv(drive_root + '/augmented-semantic-drone-dataset/class_dict.csv', index_col=False, skipinitialspace=True)\n","# class_dict_df\n","\n","label_names= list(class_dict_df.name)\n","label_codes = []\n","r= np.asarray(class_dict_df.r)\n","g= np.asarray(class_dict_df.g)\n","b= np.asarray(class_dict_df.b)\n","\n","for i in range(len(class_dict_df)):\n","    label_codes.append(tuple([r[i], g[i], b[i]]))\n","    \n","# label_codes, label_names\n","# class_dict_df"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8TUgGT4jGrCG"},"source":["## Create Useful Label & Code Conversion Dictionaries\n","These will be used for:\n","\n","* One hot encoding the mask labels for model training\n","* Decoding the predicted labels for interpretation and visualization "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":322,"status":"ok","timestamp":1681301989842,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"PkfTn3a6GrCG","scrolled":false,"trusted":true},"outputs":[],"source":["code2id = {v:k for k,v in enumerate(label_codes)}\n","id2code = {k:v for k,v in enumerate(label_codes)}\n","\n","name2id = {v:k for k,v in enumerate(label_names)}\n","id2name = {k:v for k,v in enumerate(label_names)}"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"G-uccniOGrCH"},"source":["## Define Functions for One Hot Encoding RGB Labels & Decoding Encoded Predictions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":262,"status":"ok","timestamp":1681301997714,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"COMxW7zdGrCH","scrolled":false,"trusted":true},"outputs":[],"source":["def rgb_to_onehot(rgb_image, colormap = id2code):\n","    '''Function to one hot encode RGB mask labels\n","        Inputs: \n","            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n","            colormap - dictionary of color to label id\n","        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n","    '''\n","    num_classes = len(colormap)\n","    shape = rgb_image.shape[:2]+(num_classes,)\n","    encoded_image = np.zeros( shape, dtype=np.int8 )\n","    for i, cls in enumerate(colormap):\n","        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n","    return encoded_image\n","\n","\n","def onehot_to_rgb(onehot, colormap = id2code):\n","    '''Function to decode encoded mask labels\n","        Inputs: \n","            onehot - one hot encoded image matrix (height x width x num_classes)\n","            colormap - dictionary of color to label id\n","        Output: Decoded RGB image (height x width x 3) \n","    '''\n","    single_layer = np.argmax(onehot, axis=-1)\n","    output = np.zeros( onehot.shape[:2]+(3,) )\n","    for k in colormap.keys():\n","        output[single_layer==k] = colormap[k]\n","    return np.uint8(output)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ur1-aXy5GrCI"},"source":["# Creating Custom Image Data Generators\n","## Defining Data Generators\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":213,"status":"ok","timestamp":1681302006860,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"p1c5WacsGrCI","scrolled":false,"trusted":true},"outputs":[],"source":["# Normalizing only frame images, since masks contain label info\n","data_gen_args = dict(rescale=1./255)\n","mask_gen_args = dict()\n","\n","train_frames_datagen = ImageDataGenerator(**data_gen_args)\n","train_masks_datagen = ImageDataGenerator(**mask_gen_args)\n","val_frames_datagen = ImageDataGenerator(**data_gen_args)\n","val_masks_datagen = ImageDataGenerator(**mask_gen_args)\n","\n","# Seed defined for aligning images and their masks\n","seed = 1"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QUOY020UGrCI"},"source":["# Custom Image Data Generators for Creating Batches of Frames and Masks"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":478,"status":"ok","timestamp":1681302011314,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"KadIYKwJGrCI","scrolled":false,"trusted":true},"outputs":[],"source":["def TrainAugmentGenerator(train_images_dir, train_masks_dir, seed = 1, batch_size = 8, target_size = (512, 512)):\n","    '''Train Image data generator\n","        Inputs: \n","            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n","            batch_size - number of images to import at a time\n","            train_images_dir - train images directory\n","            train_masks_dir - train masks directory\n","            target_size - tuple of integers (height, width)\n","            \n","        Output: Decoded RGB image (height x width x 3) \n","    '''\n","    train_image_generator = train_frames_datagen.flow_from_directory(\n","    train_images_dir,\n","    batch_size = batch_size, \n","    seed = seed, \n","    target_size = target_size)\n","\n","    train_mask_generator = train_masks_datagen.flow_from_directory(\n","    train_masks_dir,\n","    batch_size = batch_size, \n","    seed = seed, \n","    target_size = target_size)\n","\n","    while True:\n","        X1i = train_image_generator.next()\n","        X2i = train_mask_generator.next()\n","        \n","        #One hot encoding RGB images\n","        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n","        \n","        yield X1i[0], np.asarray(mask_encoded)\n","\n","def ValAugmentGenerator(val_images_dir, val_masks_dir, seed = 1, batch_size = 8, target_size = (512, 512)):\n","    '''Validation Image data generator\n","        Inputs: \n","            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n","            batch_size - number of images to import at a time\n","            val_images_dir - validation images directory\n","            val_masks_dir - validation masks directory\n","            target_size - tuple of integers (height, width)\n","            \n","        Output: Decoded RGB image (height x width x 3) \n","    '''\n","    val_image_generator = val_frames_datagen.flow_from_directory(\n","    val_images_dir,\n","    batch_size = batch_size, \n","    seed = seed, \n","    target_size = target_size)\n","\n","\n","    val_mask_generator = val_masks_datagen.flow_from_directory(\n","    val_masks_dir,\n","    batch_size = batch_size, \n","    seed = seed, \n","    target_size = target_size)\n","\n","\n","    while True:\n","        X1i = val_image_generator.next()\n","        X2i = val_mask_generator.next()\n","        \n","        #One hot encoding RGB images\n","        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n","        \n","        yield X1i[0], np.asarray(mask_encoded)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DdJWrQRPGrCJ"},"source":["# Function to Create U-Net Model Using VGG-16 Pre-Trained Weights\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHtbIqepGrCJ","scrolled":false,"trusted":true},"outputs":[],"source":["# !mkdir pretrained_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15664,"status":"ok","timestamp":1681301673939,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"CaYAIVJ4GrCJ","outputId":"fc53c80f-8f42-40c6-d9db-d9869f5586d9","scrolled":false,"trusted":true},"outputs":[],"source":["#if not downloaded already\n","# !wget -O pretrained_weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5  https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":635,"status":"ok","timestamp":1681302030633,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"lGqaUBfLGrCJ","outputId":"6232cc34-f9b3-4a5b-fda3-59261deb30b5","scrolled":false,"trusted":true},"outputs":[],"source":["#training parameters\n","batch_size = 32 #reduce this if you run out of memory\n","num_train_samples = len(np.sort(os.listdir(train_images+'train')))\n","num_val_samples = len(np.sort(os.listdir(val_images+'val')))\n","steps_per_epoch = np.ceil(float(num_train_samples) / float(batch_size))\n","print('steps_per_epoch: ', steps_per_epoch)\n","validation_steps = np.ceil(float(num_val_samples) / float(batch_size))\n","print('validation_steps: ', validation_steps)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Inference Starting point"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":412,"status":"ok","timestamp":1681302034710,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"LzHbvr3NGrCK","scrolled":false,"trusted":true},"outputs":[],"source":["#Create the model\n","def dice_coef(y_true, y_pred):\n","    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\n","\n","def unet(num_classes, input_shape, lr_init, vgg_weight_path=None):\n","    img_input = Input(input_shape)\n","\n","    # Block 1\n","    x = Conv2D(64, (3, 3), padding='same', name='block1_conv1')(img_input)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(64, (3, 3), padding='same', name='block1_conv2')(x)\n","    x = BatchNormalization()(x)\n","    block_1_out = Activation('relu')(x)\n","\n","    x = MaxPooling2D()(block_1_out)\n","\n","    # Block 2\n","    x = Conv2D(128, (3, 3), padding='same', name='block2_conv1')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(128, (3, 3), padding='same', name='block2_conv2')(x)\n","    x = BatchNormalization()(x)\n","    block_2_out = Activation('relu')(x)\n","\n","    x = MaxPooling2D()(block_2_out)\n","\n","    # Block 3\n","    x = Conv2D(256, (3, 3), padding='same', name='block3_conv1')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(256, (3, 3), padding='same', name='block3_conv2')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(256, (3, 3), padding='same', name='block3_conv3')(x)\n","    x = BatchNormalization()(x)\n","    block_3_out = Activation('relu')(x)\n","\n","    x = MaxPooling2D()(block_3_out)\n","\n","    # Block 4\n","    x = Conv2D(512, (3, 3), padding='same', name='block4_conv1')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(512, (3, 3), padding='same', name='block4_conv2')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(512, (3, 3), padding='same', name='block4_conv3')(x)\n","    x = BatchNormalization()(x)\n","    block_4_out = Activation('relu')(x)\n","\n","    x = MaxPooling2D()(block_4_out)\n","\n","    # Block 5\n","    x = Conv2D(512, (3, 3), padding='same', name='block5_conv1')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(512, (3, 3), padding='same', name='block5_conv2')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(512, (3, 3), padding='same', name='block5_conv3')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    for_pretrained_weight = MaxPooling2D()(x)\n","\n","    # Load pretrained weights.\n","    if vgg_weight_path is not None:\n","        vgg16 = Model(img_input, for_pretrained_weight)\n","        vgg16.load_weights(vgg_weight_path, by_name=True)\n","    \n","    # Make the layers to be loaded with vgg16 non-trainable.\n","    for layer in vgg16.layers:\n","        layer.trainable = False\n","\n","    # UP 1\n","    x = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = concatenate([x, block_4_out])\n","    x = Conv2D(512, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(512, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # UP 2\n","    x = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = concatenate([x, block_3_out])\n","    x = Conv2D(256, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(256, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # UP 3\n","    x = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = concatenate([x, block_2_out])\n","    x = Conv2D(128, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(128, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # UP 4\n","    x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = concatenate([x, block_1_out])\n","    x = Conv2D(64, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(64, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # last conv\n","    x = Conv2D(num_classes, (3, 3), activation='softmax', padding='same')(x)\n","\n","    model = Model(img_input, x)\n","    model.compile(Adam(learning_rate=lr_init),\n","                  loss='categorical_crossentropy',\n","                  metrics=[dice_coef])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IA-qgzAgGrCK","scrolled":false,"trusted":true},"outputs":[],"source":["#Load VGG16 pretrained weights\n","# shape_size = (496,496) #resolution of images after compression, 512,512 is too large for colab\n","\n","# For presereving aspect ratio of 2:3, and divisible by 16, we pick multiples of the LCM of 16 and 3\n","scale = 12 #change me for memory constraints, 12 for 384x576\n","width = 48*scale #must be divisible by 16\n","height = width*2//3\n","shape_size = (height, width)\n","print(\"shape_size: \", shape_size)\n","vgg16_unet = unet(num_classes = 24, input_shape = shape_size+(3,), lr_init = 0.0001, vgg_weight_path='./pretrained_weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vgg16_unet.summary(line_length=125, expand_nested=True, show_trainable=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":214,"status":"ok","timestamp":1681302050904,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"g_cBpCyvGrCL","scrolled":false,"trusted":true},"outputs":[],"source":["#callback functions for training\n","def exponential_decay(lr0, s):\n","    def exponential_decay_fn(epoch):\n","        return lr0 * 0.1 **(epoch / s)\n","    return exponential_decay_fn\n","\n","exponential_decay_fn = exponential_decay(0.0001, 20)\n","\n","lr_scheduler = LearningRateScheduler(\n","    exponential_decay_fn,\n","    verbose=1\n",")\n","\n","checkpoint = ModelCheckpoint(\n","    filepath = 'vgg16_unet_model_sem_drone.h5',\n","    save_best_only = True, \n","#     save_weights_only = False,\n","    monitor = 'val_loss', \n","    mode = 'auto', \n","    verbose = 1\n",")\n","\n","earlystop = EarlyStopping(\n","    monitor = 'val_loss', \n","    min_delta = 0.001, \n","    patience = 6, \n","    mode = 'auto', \n","    verbose = 1,\n","    restore_best_weights = True\n",")\n","\n","\n","csvlogger = CSVLogger(\n","    filename= \"model_training_csv.log\",\n","    separator = \",\",\n","    append = False\n",")\n","\n","callbacks = [checkpoint, earlystop, csvlogger, lr_scheduler]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#check if tf can see gpu\n","print(tf.config.list_physical_devices())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0wGIkUgZGrCL","outputId":"e37be19a-4cbb-48af-d159-7b83e6e34216","scrolled":false,"trusted":true},"outputs":[],"source":["#train model\n","history = vgg16_unet.fit(\n","    TrainAugmentGenerator(train_images_dir = train_images, train_masks_dir = train_masks, target_size = shape_size), \n","    steps_per_epoch=steps_per_epoch,\n","    validation_data = ValAugmentGenerator(val_images_dir = val_images, val_masks_dir = val_masks, target_size = shape_size), \n","    validation_steps = validation_steps, \n","    epochs = 100,\n","    callbacks=callbacks,\n","    use_multiprocessing=False,\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sq3_H88PGrCM","scrolled":false,"trusted":true},"outputs":[],"source":["with open('./trainHistoryDict', 'wb') as file_pi:\n","    pickle.dump(history.history, file_pi)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_prhcFxwGrCM","outputId":"33c3106a-a3c0-4bbf-87b6-7d4f96b42903","trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1, 3, figsize=(30, 5))\n","ax = ax.ravel()\n","metrics = ['Dice Coefficient', 'Loss', 'Learning Rate']\n","\n","for i, met in enumerate(['dice_coef', 'loss', 'lr']): \n","    if met != 'lr':\n","        ax[i].plot(history.history[met], 'o-')\n","        ax[i].plot(history.history['val_' + met], 'o-')\n","        ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n","        ax[i].set_xlabel('Epochs')\n","        ax[i].set_ylabel(metrics[i])\n","        ax[i].set_xticks(np.arange(0,21,2))\n","        ax[i].legend(['Train', 'Validation'])\n","        ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n","        ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n","    else:\n","        ax[i].plot(history.history[met], 'o-')\n","        ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n","        ax[i].set_xlabel('Epochs')\n","        ax[i].set_ylabel(metrics[i])\n","        ax[i].set_xticks(np.arange(0,21,2))\n","        ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n","        ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n","        \n","plt.savefig('model_metrics_plot.png', facecolor= 'w',transparent= False, bbox_inches= 'tight', dpi= 150)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3v_akDCGrCM","scrolled":false,"trusted":true},"outputs":[],"source":["vgg16_unet.load_weights(\"./vgg16_unet_model_sem_drone.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJS_VHNsGrCN","scrolled":false,"trusted":true},"outputs":[],"source":["demo_dir = '/augmented-semantic-drone-dataset/demo_img/demo/'\n","demo_img_names = [\"demo_img.jpg\", \"demo_img2.jpg\", \"demo_img3.jpg\", \"demo_img4.jpg\"]\n","for demo_img_name in demo_img_names:\n","    inference_img = cv2.imread(drive_root + demo_dir + demo_img_name)\n","    inference_img = cv2.cvtColor(inference_img,cv2.COLOR_BGR2RGB)\n","    inference_img = cv2.resize(inference_img, (width, height), interpolation = cv2.INTER_NEAREST)\n","    inference_img = inference_img[np.newaxis,...]*1./255\n","    print(inference_img.shape)\n","    pred = vgg16_unet.predict(inference_img)\n","    np.shape(pred)\n","\n","\n","    fig = plt.figure(figsize=(20,8))\n","\n","    ax1 = fig.add_subplot(1,2,1)\n","    ax1.imshow(inference_img[0])\n","    ax1.title.set_text('Original Image')\n","    ax1.grid(False)\n","\n","\n","    ax3 = fig.add_subplot(1,2,2)\n","    ax3.set_title('Predicted Mask')\n","    ax3.imshow(onehot_to_rgb(pred[0],id2code))\n","    ax3.grid(False)\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('subsets: ', os.listdir( drive_root + '/data-map2dfusion/data/'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Inference image splitting to increase the resolution\n","for subset in os.listdir( drive_root + '/data-map2dfusion/data/'):\n","    print('subset: ', subset)\n","    full_size_dir = f'/data-map2dfusion/data/{subset}/rgb/'\n","    split_dir = f'/data-map2dfusion/data/{subset}/rgb_split_overlap/'\n","    if not os.path.exists(drive_root + split_dir):\n","        os.makedirs(drive_root + split_dir)\n","    full_size_img_names = os.listdir(drive_root + full_size_dir)\n","    full_size_img_names.sort()\n","    overlap_px = 100\n","\n","    # divide each image into 4 overlapping patches\n","    for full_size_img_name in full_size_img_names:\n","        if not full_size_img_name.endswith('.jpg'):\n","            continue\n","        # full_size_img_name = '1459661063.910500.jpg' #test image\n","        img = cv2.imread(drive_root + full_size_dir + full_size_img_name)\n","        M,N = img.shape[0]//tile_count+ overlap_px, img.shape[1]//tile_count+ overlap_px\n","        tile1 = img[0:M,0:N]\n","        tile2 = img[0:M,-N:]\n","        tile3 = img[-M:,0:N]\n","        tile4 = img[-M:,-N:]\n","        tiles = [tile1,tile2,tile3,tile4]\n","        for i,tile in enumerate(tiles):\n","            cv2.imwrite(drive_root + split_dir + full_size_img_name[:-4] + '_' + str(i) + '.jpg', tile)\n","        # break\n","\n","\n","    \n","\n","\n","\n","\n","'''\n","# divide each image into 4 tiles\n","\n","for full_size_img_name in full_size_img_names:\n","    # full_size_img_name = '1459661063.910500.jpg' #test image\n","    img = cv2.imread(drive_root + full_size_dir + full_size_img_name)\n","    M,N = img.shape[0]//tile_count, img.shape[1]//tile_count\n","    tiles = [img[x:x+M,y:y+N] for x in range(0,img.shape[0],M) for y in range(0,img.shape[1],N)]\n","    for i,tile in enumerate(tiles):\n","        cv2.imwrite(drive_root + split_dir + full_size_img_name[:-4] + '_' + str(i) + '.jpg', tile)\n","    # break\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## For overlapping tiles\n","tile_count = 2\n","for subset in os.listdir( drive_root + '/data-map2dfusion/data/'):\n","    pred_datasets_dir = './data-map2dfusion/data'\n","    # pred_datasets_dir = os.path.expanduser('~') + '/data/3d_vision/'\n","    if subset == 'phantom3-factory-kfs':\n","        continue\n","    pred_dataset = subset\n","    print('pred_dataset: ', pred_dataset)\n","    pred_images_dir = os.path.join(pred_datasets_dir, pred_dataset, 'rgb_split_overlap')\n","    tiles = []\n","    # save to inference folder\n","    segmentation_dir = os.path.join(pred_datasets_dir, pred_dataset, 'sem')\n","    if not os.path.exists(segmentation_dir):\n","        os.makedirs(segmentation_dir)\n","\n","    original_height, original_width, _ = 1080, 1920, 3\n","    tile_height, tile_width = 540, 960\n","    for img in sorted(os.listdir(pred_images_dir)):\n","        if not img.endswith('.jpg'):\n","            continue\n","        inference_img = cv2.imread(os.path.join(pred_images_dir, img))\n","        inference_img = cv2.cvtColor(inference_img,cv2.COLOR_BGR2RGB)\n","        inference_img = cv2.resize(inference_img, (width, height), interpolation = cv2.INTER_NEAREST)\n","        inference_img = inference_img[np.newaxis,...]*1./255\n","        pred = vgg16_unet.predict(inference_img)\n","        segmented_img = onehot_to_rgb(pred[0],id2code)\n","        segmented_img =cv2.cvtColor(segmented_img,cv2.COLOR_RGB2BGR)\n","        # #save tile img\n","        # cv2.imwrite(os.path.join(segmentation_dir, img[:-4] + '.png'), segmented_img)\n","\n","        #resize to original tile size with overlap\n","        segmented_img = cv2.resize(segmented_img, (tile_width + overlap_px, tile_height + overlap_px), interpolation = cv2.INTER_NEAREST)\n","\n","        tiles.append(segmented_img)\n","        if len(tiles) == tile_count**2:\n","            stack = []\n","            for i in range(tile_count):\n","                stack.append(np.hstack(tiles[i*tile_count:(i+1)*tile_count]))\n","            segmented_img_overlapped = np.vstack(stack)\n","            # #save overlapped tile img\n","            # cv2.imwrite(os.path.join(segmentation_dir, img[:-4] + '_overlapped.png'), segmented_img_overlapped)\n","\n","            segmented_img = np.delete(segmented_img_overlapped, np.s_[tile_height: -tile_height], axis=0)\n","            segmented_img = np.delete(segmented_img, np.s_[tile_width: -tile_width], axis=1)\n","\n","            img_name = img[:-6] + '.png'\n","            #save merged img\n","            cv2.imwrite(os.path.join(segmentation_dir, img_name), segmented_img)\n","            tiles = []"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.15"}},"nbformat":4,"nbformat_minor":0}
