{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"qWWorEfvGrB1"},"source":["# UAVid Drone Dataset\n","\n","\n","UAVid Dataset\n","The The UAVid dataset is an UAV video dataset for semantic segmentation task focusing on urban scenes. It has several features:\n","\n","Semantic segmentation 4K resolution UAV videos 8 object categories Street scene context.\n","\n","The segmentation categories need to be converted to the format of the Semantic Drone Dataset, as seen below\n","\n","## Semantic Annotation\n","\n","The images are labeled densely using polygons and contain the following 24 classes: \n","  - unlabeled\n","  - paved-area\n","  - dirt\n","  - grass\n","  - gravel\n","  - water\n","  - rocks\n","  - pool\n","  - vegetation\n","  - roof\n","  - wall\n","  - window\n","  - door\n","  - fence\n","  - fence-pole\n","  - person\n","  - dog\n","  - car\n","  - bicycle\n","  - tree\n","  - bald-tree\n","  - ar-marker\n","  - obstacle\n","  - conflicting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2533,"status":"ok","timestamp":1681301928982,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"6EYunIkNHIBe","outputId":"038a6703-a352-4d1c-a8b8-169d918b1e53"},"outputs":[],"source":["#run if using google colab\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":320,"status":"ok","timestamp":1681301930744,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"1bp3cPBIJ5Wz"},"outputs":[],"source":["# drive_root = \"G:/MyDrive/3DV/drone-images-semantic-segmentation/\" #if running on google colab\n","drive_root = \".\" #if running locally"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":373,"status":"ok","timestamp":1681301938760,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"oZGGn97gONyc","outputId":"06422903-65b9-442d-85af-b1f303c427ab"},"outputs":[],"source":["cd /content/drive/MyDrive/3DV/drone-images-semantic-segmentation/ #confirm in correct location on colab"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3668,"status":"ok","timestamp":1681301944548,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"8wCHkWIWGrB8","scrolled":false,"trusted":true},"outputs":[],"source":["import cv2\n","import random\n","import albumentations as A\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import os"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"y9CkscNCGrB8"},"source":["# Data Augmentation using Albumentations Library\n","\n","Perfromed as described in README.md"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Data Preparation\n","Slow, rather download from shared drive than recreate locally"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6pIcvFSqGrB-","scrolled":false,"trusted":true},"outputs":[],"source":["transform = A.Compose([\n","    A.RandomCrop(width=2400, height=1600, p=1.0),\n","    A.HorizontalFlip(p=1.0),\n","    A.VerticalFlip(p=1.0),\n","    A.Rotate(limit=[60, 240], p=1.0, interpolation=cv2.INTER_NEAREST),\n","    A.RandomBrightnessContrast(brightness_limit=[-0.2, 0.4], contrast_limit=0.2, p=1.0),\n","    A.OneOf([\n","        A.CLAHE (clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n","        A.GridDistortion(p=0.5),\n","        A.OpticalDistortion(distort_limit=1, shift_limit=0.5, interpolation=cv2.INTER_NEAREST, p=0.5),\n","    ], p=1.0),\n","], p=1.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmtO2CGgGrB-","scrolled":false,"trusted":true},"outputs":[],"source":["def visualize(image, mask, original_image=None, original_mask=None):\n","    fontsize = 16\n","\n","    if original_image is None and original_mask is None:\n","        f, ax = plt.subplots(2, 1, figsize=(16, 16), squeeze=True)\n","        f.set_tight_layout(h_pad=5, w_pad=5)\n","\n","        ax[0].imshow(image)\n","        ax[1].imshow(mask)\n","    else:\n","        f, ax = plt.subplots(2, 2, figsize=(16, 16), squeeze=True)\n","        plt.tight_layout(pad=0.2, w_pad=1.0, h_pad=0.01)\n","\n","        ax[0, 0].imshow(original_image)\n","        ax[0, 0].set_title('Original Image', fontsize=fontsize)\n","\n","        ax[1, 0].imshow(original_mask)\n","        ax[1, 0].set_title('Original Mask', fontsize=fontsize)\n","\n","        ax[0, 1].imshow(image)\n","        ax[0, 1].set_title('Transformed Image', fontsize=fontsize)\n","\n","        ax[1, 1].imshow(mask)\n","        ax[1, 1].set_title('Transformed Mask', fontsize=fontsize)\n","        \n","    plt.savefig('sample_augmented_image.png', facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1680191592071,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"ITPsUaY2KI-z","outputId":"41cee812-5a52-42dc-f13c-0d1c99b3c89c"},"outputs":[],"source":["drive_root + \"uavid_v1.5_official_release_image/train/seq1/Images/000000.png\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGZNZKscGrB_","scrolled":false,"trusted":true},"outputs":[],"source":["image = cv2.imread(\"./\" + \"uavid_v1.5_official_release_image/train/seq1/Images/000000.png\")\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","mask = cv2.imread(\"./\" + \"uavid_v1.5_official_release_image/train/seq1/Labels_SDD/000000.png\")\n","mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB\n","                   )\n","\n","transformed = transform(image=image, mask=mask)\n","transformed_image = transformed['image']\n","transformed_mask = transformed['mask']\n","\n","cv2.imwrite('./image.png',cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))\n","cv2.imwrite('./mask.png',cv2.cvtColor(transformed_mask, cv2.COLOR_BGR2RGB))\n","\n","visualize(transformed_image, transformed_mask, image, mask)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Collate all Images from UAVid Dataset\n","import os\n","import shutil\n","\n","root_dir = './uavid_v1.5_official_release_image/train'  # Set this to the path of your root directory containing the 'seq' folders\n","\n","count = 0\n","for seq_dir in os.listdir(root_dir):\n","    if not os.path.isdir(os.path.join(root_dir, seq_dir)):\n","        continue\n","\n","    images_dir = os.path.join(root_dir, seq_dir, 'Images')\n","    labels_sdd_dir = os.path.join(root_dir, seq_dir, 'Labels_SDD')\n","\n","    # Create output directories\n","    images_out_dir = os.path.join(root_dir, 'All_Images')\n","    labels_sdd_out_dir = os.path.join(root_dir, 'All_Labels_SDD')\n","    os.makedirs(images_out_dir, exist_ok=True)\n","    os.makedirs(labels_sdd_out_dir, exist_ok=True)\n","\n","    # Copy all images and labels to output directories\n","    for file in os.listdir(images_dir):\n","        shutil.copy(os.path.join(images_dir, file), os.path.join(images_out_dir, '{:03d}.png'.format(count)))\n","        shutil.copy(os.path.join(labels_sdd_dir, file), os.path.join(labels_sdd_out_dir, '{:03d}.png'.format(count)))\n","        count += 1"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2HN_ZLbaGrCA"},"source":["## Saving Augmented Images to Disk\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":498,"status":"ok","timestamp":1681290677047,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"CcyFluiFGrCA","outputId":"6b340004-064e-4d6c-bd38-95811a9e6d84","scrolled":false,"trusted":true},"outputs":[],"source":["!mkdir aug_images\n","!mkdir aug_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1681290855224,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"WVxrDnTqOyk7","outputId":"4d9e4d87-e3bc-40b0-eca8-14cfe673d496"},"outputs":[],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hO5D1G2N1HMG"},"outputs":[],"source":["!mkdir processed_dataset/train_images/\n","!mkdir processed_dataset/train_masks/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ks2YjPo224f"},"outputs":[],"source":["!mkdir processed_dataset/train_images/train\n","!mkdir processed_dataset/train_masks/train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !mkdir augmented-uavid-dataset\\\\train_images\n","!mkdir augmented-uavid-dataset\\\\train_masks\n","\n","!mkdir augmented-uavid-dataset\\\\train_images\\\\train\n","!mkdir augmented-uavid-dataset\\\\train_masks\\\\train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xP4RHrQ3GrCB","scrolled":false,"trusted":true},"outputs":[],"source":["images_dir = r'.\\uavid_v1.5_official_release_image\\train\\All_Images/'\n","masks_dir = r'.\\uavid_v1.5_official_release_image\\train\\All_Labels_SDD/'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"riWh4isTGrCB","scrolled":false,"trusted":true},"outputs":[],"source":["file_names = np.sort(os.listdir(images_dir)) \n","file_names = np.char.split(file_names, '.')\n","filenames = np.array([])\n","for i in range(len(file_names)):\n","    filenames = np.append(filenames, file_names[i][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bs-AVT3uGrCC","scrolled":false,"trusted":true},"outputs":[],"source":["\n","def augment_dataset(count):\n","    '''Function for data augmentation\n","        Input:\n","            count - total no. of images after augmentation = initial no. of images * count\n","        Output:\n","            writes augmented images (input images & segmentation masks) to the working directory\n","    '''\n","    i = 0\n","    for i in range(count):\n","        for file in filenames:\n","            #3 dec places file name format\n","            file = '{:03d}'.format(int(file))\n","            img = cv2.imread(images_dir+file+'.png')\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            mask = cv2.imread(masks_dir+file+'.png')\n","            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n","            transformed = transform(image=img, mask=mask)\n","            transformed_image = transformed['image']\n","            transformed_mask = transformed['mask']\n","\n","            cv2.imwrite('./augmented-uavid-dataset/train_images/train/aug_{}_'.format(str(i+1))+file+'.png',cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))\n","            cv2.imwrite('./augmented-uavid-dataset/train_masks/train/aug_{}_'.format(str(i+1))+file+'.png',cv2.cvtColor(transformed_mask, cv2.COLOR_BGR2RGB))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CntSzR0BGrCC","scrolled":false,"trusted":true},"outputs":[],"source":["augment_dataset(4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZWuKsP9GrCC","scrolled":false,"trusted":true},"outputs":[],"source":["!zip -r aug_images.zip './aug_images/'\n","!zip -r aug_masks.zip './aug_masks/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6p74dKwoGrCD","scrolled":false,"trusted":true},"outputs":[],"source":["!rm -rf './aug_images/'\n","!rm -rf './aug_masks/'"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"f-T1zysSGrCD"},"source":["# Working with Augmented Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20786,"status":"ok","timestamp":1681301968114,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"QntOwyURGrCD","outputId":"65cfb55a-fdd3-473c-e887-12f88bb8a41f","scrolled":false,"trusted":true},"outputs":[],"source":["import keract"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1681301968115,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"RodkZow-GrCE","scrolled":false,"trusted":true},"outputs":[],"source":["import pickle\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from IPython.display import SVG\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import os, re, sys, random, shutil, cv2\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import applications, optimizers\n","from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras.utils import model_to_dot, plot_model\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler, TensorBoard\n","from tensorflow.keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, BatchNormalization, Add, concatenate, Conv2DTranspose"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1681301970660,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"rxvF3fv5GrCE","scrolled":false,"trusted":true},"outputs":[],"source":["train_images = drive_root + \"/augmented-uavid-dataset/train_images/\"\n","# train_images = drive_root + \"semantic_drone_dataset/\"\n","train_masks = drive_root + \"/augmented-uavid-dataset/train_masks/\"\n","val_images = drive_root + \"/augmented-uavid-dataset/val_images/\"\n","val_masks = drive_root + \"/augmented-uavid-dataset/val_masks/\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Optional Vizualization of Augmented Images"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1681301972670,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"_kqg-7jqGrCE","scrolled":false,"trusted":true},"outputs":[],"source":["file_names = np.sort(os.listdir(train_images + 'train/')) \n","file_names = np.char.split(file_names, '.')\n","filenames = np.array([])\n","for i in range(len(file_names[:5])):\n","    filenames = np.append(filenames, file_names[i][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1BClok-3m1hUuR540xxp4j3nnQxD_UVp9"},"executionInfo":{"elapsed":71747,"status":"ok","timestamp":1681299632325,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"NTzjhtQlGrCE","outputId":"20f452d5-d905-4951-df41-bba0e2e15e2a","scrolled":false,"trusted":true},"outputs":[],"source":["def show_data(files, original_images_dir, label_images_dir):\n","\n","    for file in files:\n","        fig, axs = plt.subplots(1, 2, figsize=(20, 10), constrained_layout=True)\n","\n","        axs[0].imshow(Image.open(original_images_dir+'train/'+str(file)+'.jpg'))\n","        axs[0].set_title('Original Image', fontdict = {'fontsize':20})\n","        axs[0].set_xticks(np.arange(0, 6500, 1000))\n","        axs[0].set_yticks(np.arange(0, 4500, 1000))\n","        axs[0].grid(False)\n","        axs[0].axis(True)\n","\n","        semantic_label_image = Image.open(label_images_dir+ 'train/'+str(file)+'.png')\n","        semantic_label_image = np.asarray(semantic_label_image)\n","        axs[1].imshow(semantic_label_image)\n","        axs[1].set_title('Semantic Segmentation Mask', fontdict = {'fontsize':20})\n","        axs[1].set_xticks(np.arange(0, 6500, 1000))\n","        axs[1].set_yticks(np.arange(0, 4500, 1000))\n","        axs[1].grid(False)\n","        axs[1].axis(True)\n","\n","        plt.savefig('image_'+file, facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)\n","        plt.show()\n","    \n","show_data(filenames[:5], train_images, train_masks)    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iCb4x1wkGrCF","scrolled":false,"trusted":true},"outputs":[],"source":["augmented_files = ['092','118', '228', '277', '376']\n","\n","def show_augmented_images(files, original_images_dir):\n","    for file in files:\n","        fig, axs = plt.subplots(1, 5, figsize=(30, 6), constrained_layout=True)\n","        for i in range(5):\n","            if i == 0:\n","                axs[i].imshow(Image.open(original_images_dir+'train/'+str(file)+'.jpg'))\n","                axs[i].set_title('Original Image: {}.jpg'.format(file), fontdict = {'fontsize':20})\n","                axs[i].set_xticks(np.arange(0, 6500, 1000))\n","                axs[i].set_yticks(np.arange(0, 4500, 1000))\n","                axs[i].grid(False)\n","                axs[i].axis(True)\n","            else:\n","                axs[i].imshow(Image.open(original_images_dir+'train/aug_'+str(i)+'_'+str(file)+'.jpg'))\n","                axs[i].set_title('Augmented Image: aug_'+str(i)+'_'+str(file)+'.jpg', fontdict = {'fontsize':20})\n","                axs[i].set_xticks(np.arange(0, 4500, 1000))\n","                axs[i].set_yticks(np.arange(0, 3001, 1000))\n","                axs[i].grid(False)\n","                axs[i].axis(True)\n","\n","        plt.savefig('aug_image_'+file, facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)\n","        plt.show()\n","    \n","def show_augmented_masks(files, label_images_dir):\n","    for file in files:\n","        fig, axs = plt.subplots(1, 5, figsize=(30, 6), constrained_layout=True)\n","        for i in range(5):\n","            if i == 0:\n","                axs[i].imshow(Image.open(label_images_dir+'train/'+str(file)+'.png'))\n","                axs[i].set_title('Original Mask: {}.png'.format(file), fontdict = {'fontsize':20})\n","                axs[i].set_xticks(np.arange(0, 6500, 1000))\n","                axs[i].set_yticks(np.arange(0, 4500, 1000))\n","                axs[i].grid(False)\n","                axs[i].axis(True)\n","            else:\n","                axs[i].imshow(Image.open(label_images_dir+'train/aug_'+str(i)+'_'+str(file)+'.png'))\n","                axs[i].set_title('Augmented Mask: aug_'+str(i)+'_'+str(file)+'.png', fontdict = {'fontsize':20})\n","                axs[i].set_xticks(np.arange(0, 4500, 1000))\n","                axs[i].set_yticks(np.arange(0, 3001, 1000))\n","                axs[i].grid(False)\n","                axs[i].axis(True)\n","\n","        plt.savefig('aug_mask_'+file, facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":849},"executionInfo":{"elapsed":1937,"status":"error","timestamp":1681299635482,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"puwUCauVGrCF","outputId":"a3c53e58-ee3d-4625-e9fa-1d9b755dcd74","scrolled":false,"trusted":true},"outputs":[],"source":["show_augmented_images(augmented_files, train_images)\n","show_augmented_masks(augmented_files, train_masks)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Labels for Augmented Images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":801},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1681301980895,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"kFSIKWl8GrCF","outputId":"d2b7be83-3b03-488a-ef25-1ef366a61646","scrolled":false,"trusted":true},"outputs":[],"source":["class_dict_df = pd.read_csv(drive_root + '/augmented-semantic-drone-dataset/class_dict.csv', index_col=False, skipinitialspace=True)\n","# class_dict_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["label_names= list(class_dict_df.name)\n","label_codes = []\n","r= np.asarray(class_dict_df.r)\n","g= np.asarray(class_dict_df.g)\n","b= np.asarray(class_dict_df.b)\n","\n","for i in range(len(class_dict_df)):\n","    label_codes.append(tuple([r[i], g[i], b[i]]))\n","    \n","label_codes[:5], label_names[:5]"]},{"cell_type":"markdown","metadata":{"id":"8TUgGT4jGrCG"},"source":["## Create Useful Label & Code Conversion Dictionaries\n","These will be used for:\n","\n","* One hot encoding the mask labels for model training\n","* Decoding the predicted labels for interpretation and visualization "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":322,"status":"ok","timestamp":1681301989842,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"PkfTn3a6GrCG","scrolled":false,"trusted":true},"outputs":[],"source":["code2id = {v:k for k,v in enumerate(label_codes)}\n","id2code = {k:v for k,v in enumerate(label_codes)}\n","\n","name2id = {v:k for k,v in enumerate(label_names)}\n","id2name = {k:v for k,v in enumerate(label_names)}"]},{"cell_type":"markdown","metadata":{"id":"G-uccniOGrCH"},"source":["## Define Functions for One Hot Encoding RGB Labels & Decoding Encoded Predictions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":262,"status":"ok","timestamp":1681301997714,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"COMxW7zdGrCH","scrolled":false,"trusted":true},"outputs":[],"source":["def rgb_to_onehot(rgb_image, colormap = id2code):\n","    '''Function to one hot encode RGB mask labels\n","        Inputs: \n","            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n","            colormap - dictionary of color to label id\n","        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n","    '''\n","    num_classes = len(colormap)\n","    shape = rgb_image.shape[:2]+(num_classes,)\n","    encoded_image = np.zeros( shape, dtype=np.int8 )\n","    for i, cls in enumerate(colormap):\n","        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n","    return encoded_image\n","\n","\n","def onehot_to_rgb(onehot, colormap = id2code):\n","    '''Function to decode encoded mask labels\n","        Inputs: \n","            onehot - one hot encoded image matrix (height x width x num_classes)\n","            colormap - dictionary of color to label id\n","        Output: Decoded RGB image (height x width x 3) \n","    '''\n","    single_layer = np.argmax(onehot, axis=-1)\n","    output = np.zeros( onehot.shape[:2]+(3,) )\n","    for k in colormap.keys():\n","        output[single_layer==k] = colormap[k]\n","    return np.uint8(output)"]},{"cell_type":"markdown","metadata":{"id":"Ur1-aXy5GrCI"},"source":["# Creating Custom Image Data Generators\n","## Defining Data Generators\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":213,"status":"ok","timestamp":1681302006860,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"p1c5WacsGrCI","scrolled":false,"trusted":true},"outputs":[],"source":["# Normalizing only frame images, since masks contain label info\n","data_gen_args = dict(rescale=1./255)\n","mask_gen_args = dict()\n","\n","train_frames_datagen = ImageDataGenerator(**data_gen_args)\n","train_masks_datagen = ImageDataGenerator(**mask_gen_args)\n","val_frames_datagen = ImageDataGenerator(**data_gen_args)\n","val_masks_datagen = ImageDataGenerator(**mask_gen_args)\n","\n","# Seed defined for aligning images and their masks\n","seed = 1"]},{"cell_type":"markdown","metadata":{"id":"QUOY020UGrCI"},"source":["# Custom Image Data Generators for Creating Batches of Frames and Masks"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":478,"status":"ok","timestamp":1681302011314,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"KadIYKwJGrCI","scrolled":false,"trusted":true},"outputs":[],"source":["def TrainAugmentGenerator(train_images_dir, train_masks_dir, seed = 1, batch_size = 8, target_size = (512, 512)):\n","    '''Train Image data generator\n","        Inputs: \n","            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n","            batch_size - number of images to import at a time\n","            train_images_dir - train images directory\n","            train_masks_dir - train masks directory\n","            target_size - tuple of integers (height, width)\n","            \n","        Output: Decoded RGB image (height x width x 3) \n","    '''\n","    train_image_generator = train_frames_datagen.flow_from_directory(\n","    train_images_dir,\n","    batch_size = batch_size, \n","    seed = seed, \n","    target_size = target_size)\n","\n","    train_mask_generator = train_masks_datagen.flow_from_directory(\n","    train_masks_dir,\n","    batch_size = batch_size, \n","    seed = seed, \n","    target_size = target_size)\n","\n","    while True:\n","        X1i = train_image_generator.next()\n","        X2i = train_mask_generator.next()\n","        \n","        #One hot encoding RGB images\n","        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n","        \n","        yield X1i[0], np.asarray(mask_encoded)\n","\n","def ValAugmentGenerator(val_images_dir, val_masks_dir, seed = 1, batch_size = 8, target_size = (512, 512)):\n","    '''Validation Image data generator\n","        Inputs: \n","            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n","            batch_size - number of images to import at a time\n","            val_images_dir - validation images directory\n","            val_masks_dir - validation masks directory\n","            target_size - tuple of integers (height, width)\n","            \n","        Output: Decoded RGB image (height x width x 3) \n","    '''\n","    val_image_generator = val_frames_datagen.flow_from_directory(\n","    val_images_dir,\n","    batch_size = batch_size, \n","    seed = seed, \n","    target_size = target_size)\n","\n","\n","    val_mask_generator = val_masks_datagen.flow_from_directory(\n","    val_masks_dir,\n","    batch_size = batch_size, \n","    seed = seed, \n","    target_size = target_size)\n","\n","\n","    while True:\n","        X1i = val_image_generator.next()\n","        X2i = val_mask_generator.next()\n","        \n","        #One hot encoding RGB images\n","        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n","        \n","        yield X1i[0], np.asarray(mask_encoded)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DdJWrQRPGrCJ"},"source":["# Function to Create U-Net Model Using VGG-16 Pre-Trained Weights\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHtbIqepGrCJ","scrolled":false,"trusted":true},"outputs":[],"source":["!mkdir pretrained_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15664,"status":"ok","timestamp":1681301673939,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"CaYAIVJ4GrCJ","outputId":"fc53c80f-8f42-40c6-d9db-d9869f5586d9","scrolled":false,"trusted":true},"outputs":[],"source":["#if not downloaded already\n","!wget -O pretrained_weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5  https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":635,"status":"ok","timestamp":1681302030633,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"lGqaUBfLGrCJ","outputId":"6232cc34-f9b3-4a5b-fda3-59261deb30b5","scrolled":false,"trusted":true},"outputs":[],"source":["#training parameters\n","batch_size = 32 #reduce this if you run out of memory\n","num_train_samples = len(np.sort(os.listdir(train_images+'train')))\n","num_val_samples = len(np.sort(os.listdir(val_images+'val')))\n","steps_per_epoch = np.ceil(float(num_train_samples) / float(batch_size))\n","print('steps_per_epoch: ', steps_per_epoch)\n","validation_steps = np.ceil(float(num_val_samples) / float(batch_size))\n","print('validation_steps: ', validation_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":412,"status":"ok","timestamp":1681302034710,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"LzHbvr3NGrCK","scrolled":false,"trusted":true},"outputs":[],"source":["#Create the model\n","def dice_coef(y_true, y_pred):\n","    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\n","\n","def unet(num_classes, input_shape, lr_init, vgg_weight_path=None):\n","    img_input = Input(input_shape)\n","\n","    # Block 1\n","    x = Conv2D(64, (3, 3), padding='same', name='block1_conv1')(img_input)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(64, (3, 3), padding='same', name='block1_conv2')(x)\n","    x = BatchNormalization()(x)\n","    block_1_out = Activation('relu')(x)\n","\n","    x = MaxPooling2D()(block_1_out)\n","\n","    # Block 2\n","    x = Conv2D(128, (3, 3), padding='same', name='block2_conv1')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(128, (3, 3), padding='same', name='block2_conv2')(x)\n","    x = BatchNormalization()(x)\n","    block_2_out = Activation('relu')(x)\n","\n","    x = MaxPooling2D()(block_2_out)\n","\n","    # Block 3\n","    x = Conv2D(256, (3, 3), padding='same', name='block3_conv1')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(256, (3, 3), padding='same', name='block3_conv2')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(256, (3, 3), padding='same', name='block3_conv3')(x)\n","    x = BatchNormalization()(x)\n","    block_3_out = Activation('relu')(x)\n","\n","    x = MaxPooling2D()(block_3_out)\n","\n","    # Block 4\n","    x = Conv2D(512, (3, 3), padding='same', name='block4_conv1')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(512, (3, 3), padding='same', name='block4_conv2')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(512, (3, 3), padding='same', name='block4_conv3')(x)\n","    x = BatchNormalization()(x)\n","    block_4_out = Activation('relu')(x)\n","\n","    x = MaxPooling2D()(block_4_out)\n","\n","    # Block 5\n","    x = Conv2D(512, (3, 3), padding='same', name='block5_conv1')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(512, (3, 3), padding='same', name='block5_conv2')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(512, (3, 3), padding='same', name='block5_conv3')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    for_pretrained_weight = MaxPooling2D()(x)\n","\n","    # Load pretrained weights.\n","    if vgg_weight_path is not None:\n","        vgg16 = Model(img_input, for_pretrained_weight)\n","        vgg16.load_weights(vgg_weight_path, by_name=True)\n","    \n","    # Make the layers to be loaded with vgg16 non-trainable.\n","    for layer in vgg16.layers:\n","        layer.trainable = False\n","\n","    # UP 1\n","    x = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = concatenate([x, block_4_out])\n","    x = Conv2D(512, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(512, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # UP 2\n","    x = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = concatenate([x, block_3_out])\n","    x = Conv2D(256, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(256, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # UP 3\n","    x = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = concatenate([x, block_2_out])\n","    x = Conv2D(128, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(128, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # UP 4\n","    x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = concatenate([x, block_1_out])\n","    x = Conv2D(64, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(64, (3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    # last conv\n","    x = Conv2D(num_classes, (3, 3), activation='softmax', padding='same')(x)\n","\n","    model = Model(img_input, x)\n","    model.compile(Adam(learning_rate=lr_init),\n","                  loss='categorical_crossentropy',\n","                  metrics=[dice_coef])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IA-qgzAgGrCK","scrolled":false,"trusted":true},"outputs":[],"source":["#Load VGG16 pretrained weights\n","# shape_size = (496,496) #resolution of images after compression, 512,512 is too large for colab\n","\n","# For presereving aspect ratio of 2:3, and divisible by 16, we pick multiples of the LCM of 16 and 3\n","scale = 12 #change me for memory constraints, 12 for 384x576\n","width = 48*scale #must be divisible by 16\n","height = width*2//3\n","shape_size = (height, width)\n","print(\"shape_size: \", shape_size)\n","vgg16_unet = unet(num_classes = 24, input_shape = shape_size+(3,), lr_init = 0.0001, vgg_weight_path='./pretrained_weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vgg16_unet.summary(line_length=125, expand_nested=True, show_trainable=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":214,"status":"ok","timestamp":1681302050904,"user":{"displayName":"Timothy Denton","userId":"17802934146786439312"},"user_tz":-120},"id":"g_cBpCyvGrCL","scrolled":false,"trusted":true},"outputs":[],"source":["#callback functions for training\n","def exponential_decay(lr0, s):\n","    def exponential_decay_fn(epoch):\n","        return lr0 * 0.1 **(epoch / s)\n","    return exponential_decay_fn\n","\n","exponential_decay_fn = exponential_decay(0.0001, 20)\n","\n","lr_scheduler = LearningRateScheduler(\n","    exponential_decay_fn,\n","    verbose=1\n",")\n","\n","checkpoint = ModelCheckpoint(\n","    filepath = 'vgg16_unet_model_uavid.h5',\n","    save_best_only = True, \n","#     save_weights_only = False,\n","    monitor = 'val_loss', \n","    mode = 'auto', \n","    verbose = 1\n",")\n","\n","earlystop = EarlyStopping(\n","    monitor = 'val_loss', \n","    min_delta = 0.001, \n","    patience = 6, \n","    mode = 'auto', \n","    verbose = 1,\n","    restore_best_weights = True\n",")\n","\n","\n","csvlogger = CSVLogger(\n","    filename= \"model_training_csv.log\",\n","    separator = \",\",\n","    append = False\n",")\n","\n","callbacks = [checkpoint, earlystop, csvlogger, lr_scheduler]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#check if tf can see gpu\n","print(tf.config.list_physical_devices())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0wGIkUgZGrCL","outputId":"e37be19a-4cbb-48af-d159-7b83e6e34216","scrolled":false,"trusted":true},"outputs":[],"source":["#train model\n","history = vgg16_unet.fit(\n","    TrainAugmentGenerator(train_images_dir = train_images, train_masks_dir = train_masks, target_size = shape_size), \n","    steps_per_epoch=steps_per_epoch,\n","    validation_data = ValAugmentGenerator(val_images_dir = val_images, val_masks_dir = val_masks, target_size = shape_size), \n","    validation_steps = validation_steps, \n","    epochs = 100,\n","    callbacks=callbacks,\n","    use_multiprocessing=False,\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sq3_H88PGrCM","scrolled":false,"trusted":true},"outputs":[],"source":["with open('./trainHistoryDict', 'wb') as file_pi:\n","    pickle.dump(history.history, file_pi)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_prhcFxwGrCM","outputId":"33c3106a-a3c0-4bbf-87b6-7d4f96b42903","trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1, 3, figsize=(30, 5))\n","ax = ax.ravel()\n","metrics = ['Dice Coefficient', 'Loss', 'Learning Rate']\n","\n","for i, met in enumerate(['dice_coef', 'loss', 'lr']): \n","    if met != 'lr':\n","        ax[i].plot(history.history[met], 'o-')\n","        ax[i].plot(history.history['val_' + met], 'o-')\n","        ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n","        ax[i].set_xlabel('Epochs')\n","        ax[i].set_ylabel(metrics[i])\n","        ax[i].set_xticks(np.arange(0,21,2))\n","        ax[i].legend(['Train', 'Validation'])\n","        ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n","        ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n","    else:\n","        ax[i].plot(history.history[met], 'o-')\n","        ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n","        ax[i].set_xlabel('Epochs')\n","        ax[i].set_ylabel(metrics[i])\n","        ax[i].set_xticks(np.arange(0,21,2))\n","        ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n","        ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n","        \n","plt.savefig('model_metrics_plot.png', facecolor= 'w',transparent= False, bbox_inches= 'tight', dpi= 150)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3v_akDCGrCM","scrolled":false,"trusted":true},"outputs":[],"source":["vgg16_unet.load_weights(\"./vgg16_unet_model_uavid.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ncqH62GtGrCM","scrolled":false,"trusted":true},"outputs":[],"source":["testing_gen = ValAugmentGenerator(val_images_dir = val_images, val_masks_dir = val_masks, target_size = shape_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJS_VHNsGrCN","scrolled":false,"trusted":true},"outputs":[],"source":["demo_dir = '/augmented-semantic-drone-dataset/demo_img/demo/'\n","demo_img_names = [\"demo_img.jpg\", \"demo_img2.jpg\", \"demo_img3.jpg\", \"demo_img4.jpg\"]\n","for demo_img_name in demo_img_names:\n","    demo_img = cv2.imread(drive_root + demo_dir + demo_img_name)\n","    demo_img = cv2.cvtColor(demo_img,cv2.COLOR_BGR2RGB)\n","    demo_img = cv2.resize(demo_img, (width, height))\n","    demo_img = demo_img[np.newaxis,...]\n","    print(demo_img.shape)\n","    pred = vgg16_unet.predict(demo_img)\n","    np.shape(pred)\n","\n","\n","    fig = plt.figure(figsize=(20,8))\n","\n","    ax1 = fig.add_subplot(1,2,1)\n","    ax1.imshow(demo_img[0])\n","    ax1.title.set_text('Original Image')\n","    ax1.grid(False)\n","\n","\n","    ax3 = fig.add_subplot(1,2,2)\n","    ax3.set_title('Predicted Mask')\n","    ax3.imshow(onehot_to_rgb(pred[0],id2code))\n","    ax3.grid(False)\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_9dU38BGrCN","outputId":"ceb59c6c-a247-48c6-a07f-5502a2242f37","scrolled":false,"trusted":true},"outputs":[],"source":["count = 0\n","for i in range(10):\n","    batch_img,batch_mask = next(testing_gen)\n","    pred_all= vgg16_unet.predict(batch_img)\n","    np.shape(pred_all)\n","    \n","    for j in range(0,np.shape(pred_all)[0]):\n","        count += 1\n","        fig = plt.figure(figsize=(20,8))\n","\n","        ax1 = fig.add_subplot(1,3,1)\n","        ax1.imshow(batch_img[j])\n","        ax1.title.set_text('Original Image')\n","        ax1.grid(False)\n","\n","        ax2 = fig.add_subplot(1,3,2)\n","        ax2.set_title('Ground Truth Mask')\n","        ax2.imshow(onehot_to_rgb(batch_mask[j],id2code))\n","        ax2.grid(False)\n","\n","        ax3 = fig.add_subplot(1,3,3)\n","        ax3.set_title('Predicted Mask')\n","        ax3.imshow(onehot_to_rgb(pred_all[j],id2code))\n","        ax3.grid(False)\n","\n","        plt.savefig('./predictions/prediction_{}.png'.format(count), facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 200)\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXpB705hGrCN","outputId":"43a38f51-4903-407c-e9f8-02feda064dfa","scrolled":false,"trusted":true},"outputs":[],"source":["!zip -r predictions.zip './predictions/'"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":0}
